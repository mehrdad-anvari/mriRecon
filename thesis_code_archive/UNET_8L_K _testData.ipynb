{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a257e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D, Summer\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat, loadmat\n",
    "import random\n",
    "random.seed(2)\n",
    "torch.manual_seed(2)\n",
    "torch.cuda.manual_seed(2)\n",
    "np.random.seed(2)\n",
    "\n",
    "mdict = loadmat(\"C:/Users/mehrdad/KSpaceNet/PosEncode.mat\")\n",
    "## TODO: this cell should be saves seperatly and the result could be loaded at the initially\n",
    "PosEncoding = mdict[\"PositionalEncoding\"]\n",
    "\n",
    "## Load Postional Condition, Repeat it based on batch_size and Convert it Torch.Tensor\n",
    "# PosEncoding = scipy.loatmat()\n",
    "PosEncoding = torch.tensor(PosEncoding)\n",
    "# PosEncoding = PosEncoding[np.newaxis,:,:,:].repeat(batch_size,1,1,1)\n",
    "print(PosEncoding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d954a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.fftpack\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "import torch\n",
    "import glob\n",
    "from medpy.io import load, save\n",
    "import sys\n",
    "from scipy.io import savemat\n",
    "\n",
    "save_to_dir = 'J:/Anvari/MsThesis/UNET-k2/10_Percent/Gaussian/inference/'\n",
    "kmodel_dir1 = 'J:/Anvari/MsThesis/K-Space-Net2/T1_model/'\n",
    "kmodel_dir2 = 'J:/Anvari/MsThesis/K-Space-Net2/T2_model/'\n",
    "gmodel_dir = 'J:/Anvari/MsThesis/UNET-k2/10_Percent/Gaussian/'\n",
    "LINE_CLEAR = '\\x1b[2K' # <-- ANSI sequence\n",
    "train_root = 'J:\\\\Anvari\\\\proccessed_data_257\\\\train\\\\'\n",
    "validation_root = 'J:\\\\Anvari\\\\proccessed_data_257\\\\test\\\\'\n",
    "transform_2015 = transforms.Compose([transforms.ConvertImageDtype(torch.float),\n",
    "                                     transforms.Normalize(0.5, 0.5, 0.5)])\n",
    "def load_mask(mask_name):\n",
    "    dir = 'C:\\\\Users\\\\mehrdad\\\\MS_projects\\\\mask\\\\Build mask'\n",
    "    matfile = scipy.io.loadmat(dir + '\\\\{}.mat'.format(mask_name)) \n",
    "    print(type(matfile))\n",
    "    print(matfile.keys())\n",
    "    # print(matfile.get(\"__header__\"))\n",
    "    mask = np.zeros((6,257,257))\n",
    "    mask_P = matfile.get('PrimaryMask')\n",
    "    mask_S = matfile.get('SecondaryMask')\n",
    "    mask_P = np.logical_or(mask_P,np.flip(mask_P))\n",
    "    mask_S = np.logical_or(mask_S,np.flip(mask_S))\n",
    "    mask[0,:,:]=mask_S\n",
    "    mask[1,:,:]=mask_P\n",
    "    mask[2,:,:]=mask_S\n",
    "    mask[3,:,:]=mask_P\n",
    "    mask[4,:,:]=mask_S\n",
    "    mask[5,:,:]=mask_P\n",
    "    print(mask.shape)\n",
    "    print(mask.dtype)\n",
    "    print(type(mask))\n",
    "    plt.imshow(mask_P)\n",
    "    plt.title(mask_name)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.imshow(mask_S)\n",
    "    plt.title(mask_name)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.imshow(np.logical_or(mask_P,mask_S))\n",
    "    plt.title(mask_name)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return mask\n",
    "\n",
    "mask = load_mask('GaussianDistribution1DMask_10_257')\n",
    "# mask = load_mask('UniformDistribution1DMask_30_257')\n",
    "# mask = load_mask('Deterministic1DMask_40_257')\n",
    "\n",
    "\n",
    "mask_c = 1 - mask\n",
    "\n",
    "\n",
    "# def cal_fft_for_all_channels(x):\n",
    "#     fft = np.zeros_like(x).astype('complex128')\n",
    "#     for i in range(6):\n",
    "#         fft[i, :, :] = scipy.fftpack.fft2(x[i, :, :])\n",
    "#         fft[i, :, :] = scipy.fftpack.fftshift(fft[i, :, :])\n",
    "#     return np.real(fft).astype('float32'),np.imag(fft).astype('float32')\n",
    "\n",
    "\n",
    "def cal_fft_for_all_channels(x):\n",
    "    # fft = torch.zernp.zeros_like(x).astype('complex128'), dtype=torch.complex64\n",
    "    fft = torch.zeros_like(x,dtype=torch.complex64)\n",
    "    fft = torch.fft.fft2(x)\n",
    "    fft = torch.fft.fftshift(fft,dim=(1,2))\n",
    "    # for i in range(6):\n",
    "    #     fft[i, :, :] = scipy.fftpack.fft2(x[i, :, :])\n",
    "    #     fft[i, :, :] = scipy.fftpack.fftshift(fft[i, :, :])\n",
    "    return torch.real(fft).to(torch.float32) ,torch.imag(fft).to(torch.float32)\n",
    "\n",
    "def to_bad_img(x, mask):\n",
    "    x = (x + 1.) / 2.\n",
    "    for i in range(6):\n",
    "        fft = scipy.fftpack.fft2(x[i, :, :])\n",
    "        fft = scipy.fftpack.fftshift(fft)\n",
    "        fft = fft * mask[i,:,:]\n",
    "        fft = scipy.fftpack.ifftshift(fft)\n",
    "        x[:, :, i] = scipy.fftpack.ifft2(fft)\n",
    "        x[:, :, i] = np.abs(x[:, :, i])\n",
    "        x[:, :, i] = x[:, :, i] * 2 - 1\n",
    "    return x\n",
    "\n",
    "\n",
    "class Brats2013_2D(Dataset):\n",
    "    def __init__(self, root, transform=transform_2015 , PE = PosEncoding):\n",
    "        self.img_dir = root\n",
    "        self.positionalEncoding = PE\n",
    "        self.transform = transform\n",
    "        file_list = glob.glob(root + \"*.mha\")\n",
    "        self.data = []\n",
    "        for file_path in file_list:\n",
    "            layer_name = file_path.split(\"\\\\\")[-1].split(\".\")[-2].split(\"_\")[-1]\n",
    "            self.data.append([file_path, int(layer_name)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, LayerNum = self.data[idx]\n",
    "        image, image_header = load(img_path)\n",
    "        image = torch.tensor(image).cuda().to(torch.float32)\n",
    "        image_fft_r,image_fft_i = cal_fft_for_all_channels(image)\n",
    "        image_fft_r = torch.tensor(image_fft_r)\n",
    "        image_fft_i = torch.tensor(image_fft_i)\n",
    "        image = torch.tensor(image)\n",
    "        return image_fft_r, image_fft_i, image, torch.tensor(LayerNum), torch.tensor(self.positionalEncoding)\n",
    "\n",
    "\n",
    "def imshow(img, batch_size, name, cmap):\n",
    "    npimg = img.numpy()\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    plt.figure(figsize=(4, 4))\n",
    "    if cmap != 'gray':\n",
    "        # c = 1 / np.log(1 + np.max(npimg[:,:,0]))\n",
    "        # image = c * (np.log(npimg[:,:,0] + 1))\n",
    "        image = npimg[:,:,0]\n",
    "    else:\n",
    "        image = npimg[:,:,0]\n",
    "\n",
    "    # cm = plt.get_cmap(cmap)\n",
    "    # npimg = cm(image)\n",
    "    if cmap != 'gray':\n",
    "        image[-1,-1] = 0.1\n",
    "        image[-2,-1] = 0\n",
    "        plt.imshow(image,interpolation = 'none',cmap=cmap,vmin=0,vmax=0.1)\n",
    "        plt.colorbar(orientation=\"vertical\")\n",
    "        plt.axis('off')\n",
    "        plt.savefig(save_to_dir+'{}_01.png'.format(name), format='png', dpi=600)\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        image[-1,-1] = 1\n",
    "        image[-2,-1] = 0\n",
    "        plt.imshow(image,interpolation = 'none',cmap=cmap,vmin=0,vmax=1)\n",
    "        plt.colorbar(orientation=\"vertical\")\n",
    "        plt.axis('off')\n",
    "        plt.savefig(save_to_dir+'{}_10.png'.format(name), format='png', dpi=600)\n",
    "    else:\n",
    "        image[-1,-1] = 1\n",
    "        image[-2,-1] = 0\n",
    "        plt.imshow(image,interpolation = 'none',cmap=cmap,vmin=0,vmax=1)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(save_to_dir+'{}.png'.format(name), format='png', dpi=600)\n",
    "        \n",
    "    \n",
    "    # plt.title('Batch-size = {}'.format(batch_size))\n",
    "    \n",
    "\n",
    "\n",
    "def imsave(img,epoch,i,name,batch_size):\n",
    "    img_slices = np.zeros((6*batch_size,240,240))\n",
    "    for j in range(batch_size*6):\n",
    "        img_slices[j,:,:] = img[int(j//6),int(j%6),:,:]\n",
    "    save(img_slices,save_to_dir+f'{name}_{epoch}_{i}.mha')\n",
    "    \n",
    "batch_size = 10\n",
    "# dataset = Brats2013_2D(root=train_root, PE= PosEncoding)\n",
    "dataset_val = Brats2013_2D(root=validation_root , PE= PosEncoding)\n",
    "# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, pin_memory=False,drop_last=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True,pin_memory=False,drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d1e11a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "if torch.cuda.is_available():\n",
    "    print('it works')\n",
    "else:\n",
    "    print('it doesnt work')\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and 1 > 0) else \"cpu\")\n",
    "print('device is {}'.format(device))\n",
    "\n",
    "class u_net_bn(nn.Module):\n",
    "    def __init__(self,ngpu):\n",
    "        super(u_net_bn, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.df_dim = 64\n",
    "        self.conv1 = nn.Conv2d(2, self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(1 * self.df_dim, 2 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.conv3 = nn.Conv2d(2 * self.df_dim, 4 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.conv4 = nn.Conv2d(4 * self.df_dim, 8 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.conv5 = nn.Conv2d(8 * self.df_dim, 8 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.conv6 = nn.Conv2d(8 * self.df_dim, 8 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.conv7 = nn.Conv2d(8 * self.df_dim, 8 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.conv8 = nn.Conv2d(8 * self.df_dim, 8 * self.df_dim, (2, 2), (2, 2))\n",
    "        self.up7 = nn.ConvTranspose2d(8 * self.df_dim, 8 * self.df_dim, (2, 2), stride=(2, 2), padding=(0, 0))\n",
    "        self.up6 = nn.ConvTranspose2d(16 * self.df_dim, 16 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.up5 = nn.ConvTranspose2d(24 * self.df_dim, 16 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.up4 = nn.ConvTranspose2d(24 * self.df_dim, 16 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.up3 = nn.ConvTranspose2d(24 * self.df_dim, 4 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.up2 = nn.ConvTranspose2d(8 * self.df_dim, 2 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.up1 = nn.ConvTranspose2d(4 * self.df_dim, 1 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "        self.up0 = nn.ConvTranspose2d(2 * self.df_dim, 1 * self.df_dim, (4, 4), (2, 2), padding=(1, 1))\n",
    "\n",
    "        self.batch_conv2 = nn.BatchNorm2d(2 * self.df_dim)\n",
    "        self.batch_conv3 = nn.BatchNorm2d(4 * self.df_dim)\n",
    "        self.batch_conv4 = nn.BatchNorm2d(8 * self.df_dim)\n",
    "        self.batch_conv5 = nn.BatchNorm2d(8 * self.df_dim)\n",
    "        self.batch_conv6 = nn.BatchNorm2d(8 * self.df_dim)\n",
    "        self.batch_conv7 = nn.BatchNorm2d(8 * self.df_dim)\n",
    "        self.batch_up0 = nn.BatchNorm2d(1 * self.df_dim)\n",
    "        self.batch_up1 = nn.BatchNorm2d(1 * self.df_dim)\n",
    "        self.batch_up2 = nn.BatchNorm2d(2 * self.df_dim)\n",
    "        self.batch_up3 = nn.BatchNorm2d(4 * self.df_dim)\n",
    "        self.batch_up4 = nn.BatchNorm2d(16 * self.df_dim)\n",
    "        self.batch_up5 = nn.BatchNorm2d(16 * self.df_dim)\n",
    "        self.batch_up6 = nn.BatchNorm2d(16 * self.df_dim)\n",
    "        self.batch_up7 = nn.BatchNorm2d(8 * self.df_dim)\n",
    "\n",
    "        self.outLayer = nn.Conv2d(self.df_dim, 1, (1, 1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        c1 = nn.LeakyReLU(0.2)(x)\n",
    "#         print(f'c1:{c1.shape}')\n",
    "        \n",
    "        x = self.conv2(x)\n",
    "        x = self.batch_conv2(x)\n",
    "        c2 = nn.LeakyReLU(0.2)(x)\n",
    "#         print(f'c2:{c2.shape}')\n",
    "        \n",
    "        x = self.conv3(c2)\n",
    "        x = self.batch_conv3(x)\n",
    "        c3 = nn.LeakyReLU(0.2)(x)\n",
    "#         print(f'c3:{c3.shape}')\n",
    "        \n",
    "        x = self.conv4(x)\n",
    "        x = self.batch_conv4(x)\n",
    "        c4 = nn.LeakyReLU(0.2)(x)\n",
    "#         print(f'c4:{c4.shape}')\n",
    "        \n",
    "        x = self.conv5(c4)\n",
    "        x = self.batch_conv5(x)\n",
    "        c5 = nn.LeakyReLU(0.2)(x)\n",
    "#         print(f'c5:{c5.shape}')\n",
    "        \n",
    "        x = self.conv6(c5)\n",
    "        x = self.batch_conv6(x)\n",
    "        c6 = nn.LeakyReLU(0.2)(x)\n",
    "#         print(f'c6:{c6.shape}')\n",
    "        c7 = self.conv7(c6)\n",
    "        c7 = self.batch_conv7(c7)\n",
    "#         print(f'c7:{c7.shape}')\n",
    "        x = nn.LeakyReLU(0.2)(c7)\n",
    "        x = self.conv8(x)\n",
    "        # x = self.batch_conv8(x)\n",
    "        # print(f'c8:{x.shape}')\n",
    "        u7 = self.up7(x)\n",
    "        u7 = self.batch_up7(u7)\n",
    "        u7 = nn.ReLU()(u7)\n",
    "#         print(f'u7:{u7.shape}')\n",
    "        x = torch.concat((u7, c7), dim=1)\n",
    "#         print(f'x:{x.shape}')\n",
    "        x = self.up6(x)\n",
    "        u6 = self.batch_up6(x)\n",
    "        u6 = nn.ReLU()(u6)\n",
    "#         print(f'u6:{u6.shape}')\n",
    "        x = torch.concat((u6, c6), dim=1)\n",
    "        x = self.up5(x)\n",
    "        x = self.batch_up5(x)\n",
    "        u5 = nn.ReLU()(x)\n",
    "#         print(f'u5:{u5.shape}')\n",
    "    \n",
    "        x = torch.concat((u5, c5), dim=1)\n",
    "        x = self.up4(x)\n",
    "        x = self.batch_up4(x)\n",
    "        u4 = nn.ReLU()(x)\n",
    "#         print(f'u4:{u4.shape}')\n",
    "        \n",
    "        x = torch.concat((u4, c4), dim=1)\n",
    "        x = self.up3(x)\n",
    "        x = self.batch_up3(x)\n",
    "        u3 = nn.ReLU()(x)\n",
    "#         print(f'u3:{u3.shape}')\n",
    "        \n",
    "        x = torch.concat((u3, c3), dim=1)\n",
    "        x = self.up2(x)\n",
    "        x = self.batch_up2(x)\n",
    "        u2 = nn.ReLU()(x)\n",
    "        \n",
    "        x = torch.concat((u2, c2), dim=1)\n",
    "        x = self.up1(x)\n",
    "        x = self.batch_up1(x)\n",
    "        u1 = nn.ReLU()(x)\n",
    "        \n",
    "        x = torch.concat((u1, c1), dim=1)\n",
    "        x = self.up0(x)\n",
    "        x = self.batch_up0(x)\n",
    "        p2d = (0, 1, 0, 1) \n",
    "        x = F.pad(x, p2d, \"constant\", 0)\n",
    "        x = nn.ReLU()(x)\n",
    "\n",
    "        x = self.outLayer(x)\n",
    "        x = nn.Tanh()(x)\n",
    "        # print(f'u3:{x.shape}')\n",
    "        return x\n",
    "\n",
    "netG = u_net_bn(1).to(device)\n",
    "summary(netG, input_size=(2, 257, 257))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432a75c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, layer):\n",
    "        device = layer.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = layer[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db185cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "from einops import rearrange\n",
    "if torch.cuda.is_available():\n",
    "    print('it works')\n",
    "else:\n",
    "    print('it doesnt work')\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and 1 > 0) else \"cpu\")\n",
    "print('device is {}'.format(device))\n",
    "\n",
    "\n",
    "class KspaceNetT1(nn.Module):\n",
    "    def __init__(self,ngpu):\n",
    "        super(KspaceNetT1, self).__init__()\n",
    "        self.df_dim = 32     ## TODO: normalizing the input k-space\n",
    "        self.posEn_dim = 34\n",
    "        self.layEn_dim = 32\n",
    "        self.conv1 = nn.Conv2d(10 + self.posEn_dim + self.layEn_dim , 4*self.df_dim, (1, 1), padding=(0, 0))\n",
    "        self.conv2 = nn.Conv2d(4*self.df_dim, 2*self.df_dim, (1, 1), padding=(0, 0))\n",
    "        self.conv3 = nn.Conv2d(2*self.df_dim, 1*self.df_dim, (1, 1), padding=(0, 0))\n",
    "        self.conv4 = nn.Conv2d(1*self.df_dim, 2, (1, 1), padding=(0, 0))\n",
    "\n",
    "        self.batch1 = nn.BatchNorm2d(4 * self.df_dim)\n",
    "        self.batch2 = nn.BatchNorm2d(2 * self.df_dim)\n",
    "        self.batch3 = nn.BatchNorm2d(1 * self.df_dim)\n",
    "\n",
    "        ## TODO: an MLP model for Layer encoding \n",
    "        self.layer_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(self.layEn_dim),\n",
    "            nn.Linear(self.layEn_dim, self.layEn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.layEn_dim, self.layEn_dim),\n",
    "        )\n",
    "        \n",
    "        self.layer_mlp2 = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(self.layEn_dim),\n",
    "            nn.Linear(self.layEn_dim, self.layEn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.layEn_dim, 2*self.posEn_dim),\n",
    "        )\n",
    "        \n",
    "        self.layer_mlp3 = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(self.layEn_dim),\n",
    "            nn.Linear(self.layEn_dim, self.layEn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.layEn_dim, 10),\n",
    "        )\n",
    "        \n",
    "        self.convLEPEforX = nn.Conv2d(self.posEn_dim + 10, 10*2, (1, 1), padding=(0, 0))\n",
    "        \n",
    "    def forward(self, x, PE, LE):\n",
    "\n",
    "        layerEmbeddingLatent = self.layer_mlp(LE)\n",
    "        \n",
    "        LEchs = rearrange(layerEmbeddingLatent, \"b c -> b c 1 1\")\n",
    "        LEchs = LEchs.repeat(1,1,257,257) ## TODO: check if scale and shifting X using LEchs is any better\n",
    "        ## TODO: Check if scale and shifting PE using LEch is any better\n",
    "        LEforPE = self.layer_mlp2(LE)\n",
    "        LEforPE2 = rearrange(LEforPE, \"b c -> b c 1 1\")\n",
    "        scale_shift = LEforPE2.chunk(2, dim=1)\n",
    "        scale, shift = scale_shift\n",
    "        PE = PE * (scale + 1) + shift\n",
    "        ## TODO: Check if scale and shiftinf PE using LEch and concat it to X while the X itself is scaled and shifted by scaled and shifted PE \n",
    "        LEforX = self.layer_mlp3(LE)\n",
    "        LEforX2 = rearrange(LEforX, \"b c -> b c 1 1\")\n",
    "        LEforX2 = LEforX2.repeat(1,1,257,257)\n",
    "        LEPEforX = torch.concat((PE, LEforX2),dim=1)\n",
    "        LEPEforX2 = self.convLEPEforX(LEPEforX)\n",
    "        scale_shift = LEPEforX2.chunk(2, dim=1)\n",
    "        scale, shift = scale_shift\n",
    "        x = x * (scale + 1) + shift\n",
    "        ## TODO: Concatinate input (x), Positional Encoding (PE), Layer Encoding (LE)\n",
    "        \n",
    "        \n",
    "        x = torch.concat((x, PE, LEchs),dim=1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.batch1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.batch2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.batch3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "#         x = nn.Tanh()(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d97aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "\n",
    "ngpu = 1\n",
    "lr = 0.0001\n",
    "lr_decay = 0.5\n",
    "decay_every = 5\n",
    "n_epoch = 6\n",
    "beta1 = 0.5\n",
    "\n",
    "print(torch.__version__)\n",
    "if torch.cuda.is_available():\n",
    "    print('it works')\n",
    "else:\n",
    "    print('it doesnt work')\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print('device is {}'.format(device))\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=True)\n",
    "torch.cuda.empty_cache()\n",
    "# torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "\n",
    "\n",
    "# print(\"Defining Dataloader...\")\n",
    "batch_size = 10\n",
    "workers = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7141a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_using_KspaceNet(image_fft_r,image_fft_i,LayerNum,model,mask,mask_c,t_c,t_p,PosEncoding):\n",
    "    for i in range(6):\n",
    "            image_fft_r[:,i,:,:] = image_fft_r[:,i,:,:].to(device) * mask[i,:,:]\n",
    "            image_fft_i[:,i,:,:] = image_fft_i[:,i,:,:].to(device) * mask[i,:,:]\n",
    "    ListConChIdxs = []\n",
    "    if t_c == 1:\n",
    "        ListConChIdxs = [0,t_c,2,3,5] ## index 1 is the T1 middle layer that should be predicted\n",
    "    if t_c == 4:\n",
    "        ListConChIdxs = [0,2,3,t_c,5]\n",
    "    image_fft_in = torch.concat((image_fft_r[:,ListConChIdxs,:,:],image_fft_i[:,ListConChIdxs,:,:]), dim = 1).to(device)\n",
    "    image_fft_out = torch.concat((image_fft_r[:,t_p,:,:][:,np.newaxis,:,:],image_fft_i[:,t_p,:,:][:,np.newaxis,:,:]), dim = 1).to(device)\n",
    "    ## Inference\n",
    "    Predition = model(image_fft_in.float(),PosEncoding.float(),LayerNum.to(device))\n",
    "    \n",
    "    # correct values of estimated image using true ones\n",
    "    Predition[:,0,:,:] = Predition[:,0,:,:] * mask_c[t_p,:,:] + image_fft_r[:,t_p,:,:].to(device) * mask[t_p,:,:]\n",
    "    Predition[:,1,:,:] = Predition[:,1,:,:] * mask_c[t_p,:,:] + image_fft_i[:,t_p,:,:].to(device) * mask[t_p,:,:]\n",
    "    \n",
    "    complexPrediction  = Predition[:,0,:,:][:,np.newaxis,:,:]+Predition[:,1,:,:][:,np.newaxis,:,:]*1j\n",
    "    complexPrediction = torch.fft.ifftshift(complexPrediction,dim=(2,3))\n",
    "    image_Predition = torch.fft.ifft2(complexPrediction)\n",
    "    image_Predition = torch.real(torch.tensor(image_Predition).float())\n",
    "    return image_Predition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3fd357a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_losses(fake1,good_imgs1,Pred_slice):\n",
    "    fake_fft2 = torch.fft.fft2(fake1)\n",
    "    good_img_fft2 = torch.fft.fft2(good_imgs1)\n",
    "\n",
    "    fake_fft2 = torch.fft.fftshift(fake_fft2) * mask_c[Pred_slice,:,:] + torch.fft.fftshift(good_img_fft2) * mask[Pred_slice,:,:]\n",
    "    fake_fft2 = torch.fft.ifftshift(fake_fft2)\n",
    "    fake1 = torch.fft.ifft2(fake_fft2).float()\n",
    "    \n",
    "    errG_fft_mse_r = torch.mean(torch.square(fake_fft2.real - good_img_fft2.real)) * 0.02\n",
    "    errG_fft_mse_i = torch.mean(torch.square(fake_fft2.imag - good_img_fft2.imag)) * 0.02\n",
    "    errG_fft = errG_fft_mse_r + errG_fft_mse_i\n",
    "\n",
    "    errG_numerator = torch.sqrt(torch.sum(torch.square(fake1 - good_imgs1)))\n",
    "    errG_denominator = torch.sqrt(torch.sum(torch.square(good_imgs1)))\n",
    "    errG_mse = (errG_numerator/errG_denominator)*15\n",
    "    \n",
    "    return errG_fft, errG_mse, fake1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4819a3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print('netG to device..')\n",
    "netG = u_net_bn(ngpu).to(device)\n",
    "netG = netG.float()\n",
    "## Define the model\n",
    "model2 = KspaceNetT1(ngpu).to(device)\n",
    "model1 = KspaceNetT1(ngpu).to(device)\n",
    "\n",
    "# print('netG weights init..')\n",
    "# netG.apply(weights_init)\n",
    "\n",
    "criterion_mse = torch.nn.MSELoss()\n",
    "\n",
    "# Setup Adam optimizers\n",
    "print('Setup Adam optimizers for G')\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "scheduler2 = optim.lr_scheduler.ExponentialLR(optimizerG, gamma=0.5)\n",
    "\n",
    "print(f'scheduler : {scheduler2.get_last_lr()}')\n",
    "## Define Metrics\n",
    "psnr = PeakSignalNoiseRatio().to(device)\n",
    "ssim = StructuralSimilarityIndexMeasure().to(device)\n",
    "\n",
    "# psnr = PeakSignalNoiseRatio(dim=(1,2,3),reduction='elementwise_mean',data_range=1).to(device)\n",
    "# ssim = StructuralSimilarityIndexMeasure(dim=(1,2,3),reduction='elementwise_mean',data_range=1).to(device)\n",
    "\n",
    "\n",
    "## Load the model if exists, pre-trained model\n",
    "\n",
    "if  os.path.isfile(gmodel_dir + 'model_unet.pt'):\n",
    "    netG = torch.load(gmodel_dir + 'model_unet.pt')\n",
    "    \n",
    "netG.eval()\n",
    "\n",
    "if  os.path.isfile(kmodel_dir2 + 'modelt2.pt'):\n",
    "    model2 = torch.load(kmodel_dir2 + 'modelt2.pt')\n",
    "    \n",
    "model2.eval()\n",
    "    \n",
    "    \n",
    "\n",
    "if  os.path.isfile(kmodel_dir1 + 'modelt1.pt'):\n",
    "    model1 = torch.load(kmodel_dir1 + 'modelt1.pt')\n",
    "    \n",
    "model1.eval()\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "G_mse_losses = []\n",
    "G_fft_mse_losses = []\n",
    "val_G_losses_mean = []\n",
    "val_G_mse_losses_mean = []\n",
    "val_G_fft_losses_mean = []\n",
    "\n",
    "\n",
    "PSNR = []\n",
    "SSIM = []\n",
    "MSE_FFT = []\n",
    "MSE_IMG = []\n",
    "NMSE = []\n",
    "iter_T = []\n",
    "\n",
    "\n",
    "Pred_slice = 1\n",
    "Pred_slice_feed = 0\n",
    "image_fft_r, image_fft_i, image, LayerNum, PosEncoding  = next(iter(dataloader_val))\n",
    "\n",
    "mask = torch.tensor(mask).to(device)\n",
    "mask_c = torch.tensor(mask_c).to(device)\n",
    "\n",
    "\n",
    "T1_pred = infer_using_KspaceNet(image_fft_r,image_fft_i,LayerNum,model=model1,mask=mask,mask_c=mask_c,t_c=4,t_p=1,PosEncoding=PosEncoding.to(device))\n",
    "T2_pred = infer_using_KspaceNet(image_fft_r,image_fft_i,LayerNum,model=model2,mask=mask,mask_c=mask_c,t_c=1,t_p=4,PosEncoding=PosEncoding.to(device))\n",
    "imshow(vutils.make_grid((T1_pred).detach().cpu(), padding=2, nrow=5, normalize=False), batch_size, 'T1_pred', cmap = 'gray')\n",
    "imshow(vutils.make_grid((T2_pred).detach().cpu(), padding=2, nrow=5, normalize=False), batch_size, 'T2_pred', cmap = 'gray')\n",
    "fix_bad_imgs  = torch.concat((T1_pred,T2_pred),dim=1).cuda()\n",
    "fix_good_imgs = image[:,Pred_slice,:,:][:,np.newaxis,:,:].cuda()\n",
    "clip = torch.ones_like(fix_good_imgs).to(device) * 0.1\n",
    "imshow(vutils.make_grid(torch.minimum(clip,torch.abs(fix_good_imgs-T1_pred)).detach().cpu(), padding=2, nrow=5, normalize=False), batch_size, 'T1_pred_diff', cmap = 'jet')\n",
    "imshow(vutils.make_grid(fix_good_imgs.detach().cpu(), padding=2, nrow=5, normalize=True), batch_size, 'ground_truth', cmap = 'gray')\n",
    "import time\n",
    "intialTime = time.time()\n",
    "\n",
    "t_iters = 0\n",
    "epoch = 0 \n",
    "model1.train(False)\n",
    "model2.train(False)\n",
    "netG.train(False)\n",
    "\n",
    "for i, data in enumerate(dataloader_val, 0):\n",
    "\n",
    "    image_fft_r, image_fft_i, image, LayerNum, PosEncoding = data\n",
    "    image = image.to(device)\n",
    "    image_fft_i = image_fft_i.to(device)\n",
    "    image_fft_r = image_fft_r.to(device)\n",
    "\n",
    "    \n",
    "    T1_pred = infer_using_KspaceNet(image_fft_r,image_fft_i,LayerNum,model=model1,mask=mask,mask_c=mask_c,t_c=4,t_p=1,PosEncoding=PosEncoding.to(device))\n",
    "    T2_pred = infer_using_KspaceNet(image_fft_r,image_fft_i,LayerNum,model=model2,mask=mask,mask_c=mask_c,t_c=1,t_p=4,PosEncoding=PosEncoding.to(device))\n",
    "\n",
    "    netG.zero_grad()\n",
    "\n",
    "    bad_imgs  = torch.concat((T1_pred,T2_pred),dim=1).cuda()\n",
    "    good_imgs = image[:,Pred_slice,:,:][:,np.newaxis,:,:].cuda()\n",
    "\n",
    "    fake = netG(bad_imgs.float()).to(device) + bad_imgs[:,Pred_slice_feed,:,:][:,np.newaxis,:,:]\n",
    "    \n",
    "    errG_fft, errG_mse, fake= cal_losses(fake,good_imgs,Pred_slice)\n",
    "    errG =  errG_mse + errG_fft\n",
    "\n",
    "    for k in range(fake.shape[0]):\n",
    "        fake = fake.detach()\n",
    "        good_imgs = good_imgs.detach()\n",
    "        errG_numerator = torch.sqrt(torch.sum(torch.square(fake[k,:,:,:] - good_imgs[k,:,:,:])))\n",
    "        errG_denominator = torch.sqrt(torch.sum(torch.square(good_imgs[k,:,:,:])))\n",
    "        errG_mse = (errG_numerator/errG_denominator)\n",
    "        MSE_IMG.append(errG_mse.detach().item())\n",
    "        MSE_FFT.append(errG_fft.detach().item())\n",
    "        PSNR.append(psnr(fake[k,:,:,:][np.newaxis,:,:,:], good_imgs[k,:,:,:][np.newaxis,:,:,:]).cpu())\n",
    "        SSIM.append(ssim(fake[k,:,:,:][np.newaxis,:,:,:], good_imgs[k,:,:,:][np.newaxis,:,:,:]).cpu())\n",
    "        NMSE.append(errG_mse.detach().item())\n",
    "        iter_T.append(t_iters)\n",
    "        print('Elapsed time = {}'.format(time.time() - intialTime))\n",
    "        intialTime = time.time()\n",
    "        print('[%d/%d][%d/%d]\\tLoss_G: %.4f, rrG_mse: %.4f, errG_fft_mse: %.4f, SSIM: %.4f, PSNR: %.4f' % (k, fake.shape[0], i,\n",
    "                    len(dataloader_val), errG.item(), MSE_IMG[-1], MSE_FFT[-1], SSIM[-1], PSNR[-1]))\n",
    "        mdic = {\"PSNR\": PSNR,\"SSIM\": SSIM,\"NMSE\": NMSE,\"MSE_FFT\": MSE_FFT,\"MSE_IMG\": MSE_IMG,'iter_T':iter_T}\n",
    "\n",
    "        savemat(save_to_dir + \"Training_Losses.mat\", mdic)\n",
    "\n",
    "                # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if len(PSNR) > 1:\n",
    "            if (PSNR[-1] > np.max(PSNR[0:-1])) or (PSNR[-1] < np.min(PSNR[0:-1])):\n",
    "                imshow(vutils.make_grid((good_imgs[k,:,:,:][np.newaxis,:,:,:]).cpu(), padding=2, nrow=5, normalize=False),\n",
    "                                        batch_size, '{}_{}_{:.4f}_{:.4f}_GT'.format(k, i, PSNR[-1], SSIM[-1]), cmap = 'gray')\n",
    "                imshow(vutils.make_grid((fake[k,:,:,:][np.newaxis,:,:,:]).cpu(), padding=2, nrow=5, normalize=False),\n",
    "                                        batch_size, '{}_{}_{:.4f}_{:.4f}_pred'.format(k, i, PSNR[-1], SSIM[-1]), cmap = 'gray')\n",
    "                imshow(vutils.make_grid(torch.abs(good_imgs[k,:,:,:][np.newaxis,:,:,:]-fake[k,:,:,:][np.newaxis,:,:,:]).cpu(), padding=2, nrow=5, normalize=False),\n",
    "                                        batch_size, '{}_{}_{:.4f}_{:.4f}_diff'.format(k, i, PSNR[-1], SSIM[-1]), cmap = 'gray')\n",
    "                imshow(vutils.make_grid(torch.abs(good_imgs[k,:,:,:][np.newaxis,:,:,:]-fake[k,:,:,:][np.newaxis,:,:,:]).cpu(), padding=2, nrow=5, normalize=False),\n",
    "                                        batch_size, '{}_{}_{:.4f}_{:.4f}_diff_10'.format(k, i, PSNR[-1], SSIM[-1]), cmap = 'jet')\n",
    "                imshow(vutils.make_grid(torch.minimum(clip[k,:,:,:][np.newaxis,:,:,:],torch.abs(good_imgs[k,:,:,:][np.newaxis,:,:,:]-fake[k,:,:,:][np.newaxis,:,:,:])).cpu(), padding=2, nrow=5, normalize=False),\n",
    "                                        batch_size, '{}_{}_{:.4f}_{:.4f}_diff_01'.format(k, i, PSNR[-1], SSIM[-1]), cmap = 'jet')                                     \n",
    "    t_iters += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda11_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd42dc5079a55ff97ff57ff9b01ace7b918ee8431bba5bd199ac8f7faef5fc7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
