{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825916c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from positional_encodings.torch_encodings import PositionalEncoding1D, PositionalEncoding2D, PositionalEncoding3D, Summer\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io import savemat\n",
    "\n",
    "p_enc_2d = PositionalEncoding2D(16)\n",
    "y = torch.zeros((1,257,257,16))\n",
    "Z = p_enc_2d(y)\n",
    "print(Z.shape) \n",
    "\n",
    "PosEncoding = np.zeros((34,257,257))\n",
    "for i in range(16):\n",
    "  PosEncoding[i,:,:] = Z[0,:,:,i]\n",
    "  \n",
    "\n",
    "e = np.mgrid[-1:1 + (1/128):(1/128),-1:1 + (1/128):(1/128)]\n",
    "y = e[0,:,:]\n",
    "x = e[1,:,:]\n",
    "print(y[127:130,127:130])\n",
    "print(x[127:130,127:130])\n",
    "y2 = 1 - y**2 \n",
    "x2 = 1 - x**2 \n",
    "PosEncoding[16,:,:] = x\n",
    "PosEncoding[17,:,:] = y\n",
    "PosEncoding[18,:,:] = x2\n",
    "PosEncoding[19,:,:] = y2\n",
    "\n",
    "for n in range(1,9):\n",
    "  PosEncoding[19 + n,:,:] = np.cos((n*x)**2+(n*y)**2)\n",
    "\n",
    "for n in range(1,4):\n",
    "  PosEncoding[27 + n,:,:] = 1-((0.1*(n)*x)**2+(0.1*(4-n)*y)**2)\n",
    "\n",
    "PosEncoding[31,:,:] = 0.2/((0.1+(x)**2)+(0.1+(y)**2))\n",
    "\n",
    "PosEncoding[32,:,:] = 1-np.abs(x)\n",
    "\n",
    "PosEncoding[33,:,:] = 1-np.abs(y)\n",
    "\n",
    "# for i in range(34):\n",
    "#     plt.imshow(PosEncoding[i,:,:])\n",
    "#     plt.show()\n",
    "\n",
    "_, axs = plt.subplots(5, 7, figsize=(10, 7.1))\n",
    "\n",
    "# axs = axs.flatten()\n",
    "for j in range(5):\n",
    "    for i in range(7):\n",
    "      if j*7 + i < 34:\n",
    "        axs[j,i].imshow(PosEncoding[j*7+i,:,:])\n",
    "      axs[j,i].axis('off')\n",
    "# plt.tight_layout()\n",
    "plt.tight_layout(pad=0.4, w_pad=0.4, h_pad=0.4)\n",
    "plt.show()\n",
    "\n",
    "mdic = {\"PositionalEncoding\": PosEncoding}\n",
    "savemat(\"PosEncode.mat\", mdic)\n",
    "## TODO: this cell should be saves seperatly and the result could be loaded at the initially\n",
    "\n",
    "\n",
    "## Load Postional Condition, Repeat it based on batch_size and Convert it Torch.Tensor\n",
    "# PosEncoding = scipy.loatmat()\n",
    "PosEncoding = torch.tensor(PosEncoding)\n",
    "# PosEncoding = PosEncoding[np.newaxis,:,:,:].repeat(batch_size,1,1,1)\n",
    "print(PosEncoding.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab1396ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.fftpack\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "import torch\n",
    "import glob\n",
    "from medpy.io import load, save\n",
    "import sys\n",
    "from scipy.io import savemat\n",
    "\n",
    "save_to_dir = 'J:/Anvari/MsThesis/K-Space-Net3/T1_model/'\n",
    "LINE_CLEAR = '\\x1b[2K' # <-- ANSI sequence\n",
    "train_root = 'J:\\\\Anvari\\\\proccessed_data_257\\\\train\\\\'\n",
    "validation_root = 'J:\\\\Anvari\\\\proccessed_data_257\\\\validation\\\\'\n",
    "transform_2015 = transforms.Compose([transforms.ConvertImageDtype(torch.float),\n",
    "                                     transforms.Normalize(0.5, 0.5, 0.5)])\n",
    "def load_mask(mask_name):\n",
    "    dir = 'C:\\\\Users\\\\mehrdad\\\\MS_projects\\\\mask\\\\Build mask'\n",
    "    matfile = scipy.io.loadmat(dir + '\\\\{}.mat'.format(mask_name)) \n",
    "    print(type(matfile))\n",
    "    print(matfile.keys())\n",
    "    # print(matfile.get(\"__header__\"))\n",
    "    mask = np.zeros((6,257,257))\n",
    "    mask_P = matfile.get('PrimaryMask')\n",
    "    mask_S = matfile.get('SecondaryMask')\n",
    "    mask_P = np.logical_or(mask_P,np.flip(mask_P))\n",
    "    mask_S = np.logical_or(mask_S,np.flip(mask_S))\n",
    "    mask[0,:,:]=mask_S\n",
    "    mask[1,:,:]=mask_P\n",
    "    mask[2,:,:]=mask_S\n",
    "    mask[3,:,:]=mask_P\n",
    "    mask[4,:,:]=mask_S\n",
    "    mask[5,:,:]=mask_P\n",
    "    # print(mask.sum() / (256 * 256))\n",
    "    print(mask.shape)\n",
    "    print(mask.dtype)\n",
    "    print(type(mask))\n",
    "    plt.imshow(mask_P)\n",
    "    plt.title(mask_name)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.imshow(mask_S)\n",
    "    plt.title(mask_name)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    plt.imshow(np.logical_or(mask_P,mask_S))\n",
    "    plt.title(mask_name)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    return mask\n",
    "\n",
    "mask = load_mask('GaussianDistribution1DMask_20_257')\n",
    "mask_c = torch.tensor(1 - mask)\n",
    "\n",
    "def cal_fft_for_all_channels(x):\n",
    "    fft = np.zeros_like(x).astype('complex128')\n",
    "    for i in range(6):\n",
    "        fft[i, :, :] = scipy.fftpack.fft2(x[i, :, :])\n",
    "        fft[i, :, :] = scipy.fftpack.fftshift(fft[i, :, :])\n",
    "    return np.real(fft).astype('float32'),np.imag(fft).astype('float32')\n",
    "\n",
    "def to_bad_img(x, mask):\n",
    "    x = (x + 1.) / 2.\n",
    "    \n",
    "#     print(f\"mask shape is:{x.shape}\")\n",
    "    for i in range(6):\n",
    "        fft = scipy.fftpack.fft2(x[i, :, :])\n",
    "        fft = scipy.fftpack.fftshift(fft)\n",
    "        fft = fft * mask[i,:,:]\n",
    "        fft = scipy.fftpack.ifftshift(fft)\n",
    "        x[i, :, :] = scipy.fftpack.ifft2(fft)\n",
    "        x[i, :, :] = np.abs(x[i, :, :])\n",
    "        x[i, :, :] = x[i, :, :] * 2 - 1\n",
    "    return x\n",
    "\n",
    "class Brats2013_2D(Dataset):\n",
    "    def __init__(self, root, transform=transform_2015 , PE = PosEncoding):\n",
    "        self.img_dir = root\n",
    "        self.positionalEncoding = PE\n",
    "        self.transform = transform\n",
    "        file_list = glob.glob(root + \"*.mha\")\n",
    "        self.data = []\n",
    "        for file_path in file_list:\n",
    "            layer_name = file_path.split(\"\\\\\")[-1].split(\".\")[-2].split(\"_\")[-1]\n",
    "            self.data.append([file_path, int(layer_name)])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, LayerNum = self.data[idx]\n",
    "        image, image_header = load(img_path)\n",
    "        image = np.float32(image)\n",
    "        # image = image[np.newaxis,:,:]\n",
    "        image_fft_r,image_fft_i = cal_fft_for_all_channels(image)\n",
    "        image_fft_r = torch.tensor(image_fft_r)\n",
    "        image_fft_i = torch.tensor(image_fft_i)\n",
    "        image = torch.tensor(image)\n",
    "        # print(image.shape)\n",
    "        ## TODO: should return fft of T1 and T2 and also the layer number\n",
    "        return image_fft_r, image_fft_i, image, torch.tensor(LayerNum), torch.tensor(self.positionalEncoding)\n",
    "\n",
    "\n",
    "def imshow(img, batch_size, name):\n",
    "    # img = img / 2 + 0.5     # unnormalize\n",
    "    npimg = img.numpy()\n",
    "#     print('npimg shape = {}'.format(npimg.shape))\n",
    "    npimg = np.transpose(npimg, (1, 2, 0))\n",
    "    # print('npimg shape = {}'.format(npimg.shape))\n",
    "    plt.figure(figsize=(7, 7))\n",
    "    plt.imshow(npimg,interpolation = 'none')\n",
    "    plt.axis('off')\n",
    "    plt.title('Dataloader Batch-size = {}'.format(batch_size))\n",
    "    plt.savefig(save_to_dir+'{}.png'.format(name), format='png', dpi=600)\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "def imsave(img,epoch,i,name,batch_size):\n",
    "    img_slices = np.zeros((6*batch_size,240,240))\n",
    "    for j in range(batch_size*6):\n",
    "        img_slices[j,:,:] = img[int(j//6),int(j%6),:,:]\n",
    "    save(img_slices,save_to_dir+f'{name}_{epoch}_{i}.mha')\n",
    "    \n",
    "\n",
    "batch_size = 10\n",
    "dataset = Brats2013_2D(root=train_root , PE = PosEncoding)\n",
    "dataset_val = Brats2013_2D(root=validation_root, PE=PosEncoding)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=batch_size, shuffle=True)\n",
    "img_fft_r, img_fft_i, img ,layer , PosEn = next(iter(dataloader))\n",
    "print(type(img_fft_r))\n",
    "print(img_fft_r.dtype)\n",
    "print('img_fft_r shape = {}'.format(img_fft_r.shape))\n",
    "print('layer = {}'.format(layer.shape))\n",
    "print('PosEn shape = {}'.format(PosEn.shape))\n",
    "plt.imshow(img_fft_r[0,0,:,:])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import math\n",
    "class SinusoidalPositionEmbeddings(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "\n",
    "    def forward(self, layer):\n",
    "        device = layer.device\n",
    "        half_dim = self.dim // 2\n",
    "        embeddings = math.log(10000) / (half_dim - 1)\n",
    "        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)\n",
    "        embeddings = layer[:, None] * embeddings[None, :]\n",
    "        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)\n",
    "        return embeddings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12262ca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "from torchsummary import summary\n",
    "from torchvision import models\n",
    "from einops import rearrange\n",
    "if torch.cuda.is_available():\n",
    "    print('it works')\n",
    "else:\n",
    "    print('it doesnt work')\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and 1 > 0) else \"cpu\")\n",
    "print('device is {}'.format(device))\n",
    "\n",
    "\n",
    "class KspaceNetT1(nn.Module):\n",
    "    def __init__(self,ngpu):\n",
    "        super(KspaceNetT1, self).__init__()\n",
    "        self.df_dim = 32     ## TODO: normalizing the input k-space\n",
    "        self.posEn_dim = 34\n",
    "        self.layEn_dim = 32\n",
    "        self.conv1 = nn.Conv2d(10 + self.posEn_dim + self.layEn_dim , 4*self.df_dim, (1, 1), padding=(0, 0))\n",
    "        self.conv2 = nn.Conv2d(4*self.df_dim, 2*self.df_dim, (1, 1), padding=(0, 0))\n",
    "        self.conv3 = nn.Conv2d(2*self.df_dim, 1*self.df_dim, (1, 1), padding=(0, 0))\n",
    "        self.conv4 = nn.Conv2d(1*self.df_dim, 2, (1, 1), padding=(0, 0))\n",
    "\n",
    "        self.batch1 = nn.BatchNorm2d(4 * self.df_dim)\n",
    "        self.batch2 = nn.BatchNorm2d(2 * self.df_dim)\n",
    "        self.batch3 = nn.BatchNorm2d(1 * self.df_dim)\n",
    "\n",
    "        ## TODO: an MLP model for Layer encoding \n",
    "        self.layer_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(self.layEn_dim),\n",
    "            nn.Linear(self.layEn_dim, self.layEn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.layEn_dim, self.layEn_dim),\n",
    "        )\n",
    "        \n",
    "        self.layer_mlp2 = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(self.layEn_dim),\n",
    "            nn.Linear(self.layEn_dim, self.layEn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.layEn_dim, 2*self.posEn_dim),\n",
    "        )\n",
    "        \n",
    "        self.layer_mlp3 = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(self.layEn_dim),\n",
    "            nn.Linear(self.layEn_dim, self.layEn_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(self.layEn_dim, 10),\n",
    "        )\n",
    "        \n",
    "        self.convLEPEforX = nn.Conv2d(self.posEn_dim + 10, 10*2, (1, 1), padding=(0, 0))\n",
    "        \n",
    "    def forward(self, x, PE, LE):\n",
    "\n",
    "        layerEmbeddingLatent = self.layer_mlp(LE)\n",
    "        \n",
    "        LEchs = rearrange(layerEmbeddingLatent, \"b c -> b c 1 1\")\n",
    "        LEchs = LEchs.repeat(1,1,257,257) ## TODO: check if scale and shifting X using LEchs is any better\n",
    "        ## TODO: Check if scale and shifting PE using LEch is any better\n",
    "        LEforPE = self.layer_mlp2(LE)\n",
    "        LEforPE2 = rearrange(LEforPE, \"b c -> b c 1 1\")\n",
    "        scale_shift = LEforPE2.chunk(2, dim=1)\n",
    "        scale, shift = scale_shift\n",
    "        PE = PE * (scale + 1) + shift\n",
    "        ## TODO: Check if scale and shiftinf PE using LEch and concat it to X while the X itself is scaled and shifted by scaled and shifted PE \n",
    "        LEforX = self.layer_mlp3(LE)\n",
    "        LEforX2 = rearrange(LEforX, \"b c -> b c 1 1\")\n",
    "        LEforX2 = LEforX2.repeat(1,1,257,257)\n",
    "        LEPEforX = torch.concat((PE, LEforX2),dim=1)\n",
    "        LEPEforX2 = self.convLEPEforX(LEPEforX)\n",
    "        scale_shift = LEPEforX2.chunk(2, dim=1)\n",
    "        scale, shift = scale_shift\n",
    "        x = x * (scale + 1) + shift\n",
    "        ## TODO: Concatinate input (x), Positional Encoding (PE), Layer Encoding (LE)\n",
    "        \n",
    "        \n",
    "        x = torch.concat((x, PE, LEchs),dim=1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.batch1(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.batch2(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = nn.LeakyReLU(0.2)(x)\n",
    "        x = self.batch3(x)\n",
    "\n",
    "        x = self.conv4(x)\n",
    "#         x = nn.Tanh()(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f488c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "import torchvision.utils as vutils\n",
    "from torchmetrics import StructuralSimilarityIndexMeasure, PeakSignalNoiseRatio\n",
    "import os\n",
    "\n",
    "ngpu = 1\n",
    "lr = 0.002\n",
    "lr_decay = 0.5\n",
    "decay_every = 5\n",
    "n_epoch = 5\n",
    "beta1 = 0.5\n",
    "batch_size = 10\n",
    "workers = 1\n",
    "\n",
    "## check if cuda is available or not?\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA is Available!')\n",
    "else:\n",
    "    print('CUDA is NOT Available!')\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "print('device is {}'.format(device))\n",
    "\n",
    "## Empety torch cuda cache\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "## Define the model\n",
    "model = KspaceNetT1(ngpu).to(device)\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(count_parameters(model))\n",
    "\n",
    "## Parameter initialization\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
    "        nn.init.constant_(m.bias.data, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Define optimizer\n",
    "print('Setup Adam optimizers for D')\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.5)\n",
    "\n",
    "## Define Loss\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "## Define Metrics\n",
    "psnr = PeakSignalNoiseRatio()\n",
    "ssim = StructuralSimilarityIndexMeasure(data_range=1.0)\n",
    "\n",
    "## Load the model if exists, pre-trained model\n",
    "# if  os.path.isfile(save_to_dir + 'modelt2.pt'):\n",
    "#     print('Loading trained model ....')\n",
    "#     torch.load(model, save_to_dir +'modelt2.pt')\n",
    "#     checkpoint = torch.load(save_to_dir + 'modelt2.pt')\n",
    "#     model.load_state_dict(checkpoint['model_state_dict'])\n",
    "#     optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "#     model.eval()\n",
    "    \n",
    "print(f'scheduler : {scheduler.get_last_lr()}')\n",
    "\n",
    "\n",
    "\n",
    "## Define some List to Keep track of the Learning \n",
    "\n",
    "#  Traing Data\n",
    "PSNR = []\n",
    "SSIM = []\n",
    "MSE_FFT = []\n",
    "MSE_IMG = []\n",
    "MSE_RealEven = []\n",
    "MSE_ImagOdd  = []\n",
    "\n",
    "#  Validation Data\n",
    "PSNR_V = []\n",
    "SSIM_V = []\n",
    "MSE_FFT_V = []\n",
    "MSE_IMG_V = []\n",
    "MSE_RealEven_V = []\n",
    "MSE_ImagOdd_V  = []\n",
    "t_p = 1\n",
    "t_c = 4\n",
    "\n",
    "mask = torch.tensor(mask).float().to(device)\n",
    "mask[[3,5],:,:] = 0\n",
    "mask[4,:,:] = 1\n",
    "for epoch in range(n_epoch):\n",
    "    # For each batch in the dataloader\n",
    "    iters = 0\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        model.train(True)\n",
    "        model.zero_grad()\n",
    "\n",
    "        # load one batch of data\n",
    "        image_fft_r, image_fft_i, image, LayerNum, PosEncoding = data         \n",
    "        image_fft_out_unmasked = torch.concat((image_fft_r[:,t_p,:,:][:,np.newaxis,:,:],image_fft_i[:,t_p,:,:][:,np.newaxis,:,:]), dim = 1).to(device)\n",
    "\n",
    "#         for i in range(6):\n",
    "#                 image_fft_r[:,i,:,:] = image_fft_r[:,i,:,:] * mask[i,:,:]\n",
    "#                 image_fft_i[:,i,:,:] = image_fft_i[:,i,:,:] * mask[i,:,:]\n",
    "#         image_fft_r = image_fft_r.to(device) * mask.to(device)\n",
    "#         image_fft_i = image_fft_i.to(device) * mask.to(device)\n",
    "                        \n",
    "        ## TODO: the resulted imaginary and real channel may not satisfies the fourier properties of real images.\n",
    "        #        check if defining a loss function that induce such properties can improve the estimation or not?\n",
    "        ## Seperate Input and Output of the network\n",
    "        # ListConChIdxs = [0,t_c,2,3,5] ## index 1 is the T1 middle layer that should be predicted\n",
    "        ListConChIdxs = [0,2,3,t_c,5] ## index 1 is the T1 middle layer that should be predicted\n",
    "        image_fft_in = torch.concat((image_fft_r[:,ListConChIdxs,:,:],image_fft_i[:,ListConChIdxs,:,:]), dim = 1).to(device)\n",
    "        image_fft_out = torch.concat((image_fft_r[:,t_p,:,:][:,np.newaxis,:,:],image_fft_i[:,t_p,:,:][:,np.newaxis,:,:]), dim = 1).to(device)\n",
    "        \n",
    "      \n",
    "        PosEncoding = PosEncoding.to(device)\n",
    "        ## Inference\n",
    "        Predition = model(image_fft_in.float(),PosEncoding.float(),LayerNum.to(device))\n",
    "        \n",
    "\n",
    "#         ## correct values of estimated image using true ones\n",
    "#         Predition[:,0,:,:] = Predition[:,0,:,:] * mask_c[t_c,:,:].to(device) + image_fft_r[:,t_p,:,:] * mask[t_p,:,:]\n",
    "#         Predition[:,1,:,:] = Predition[:,1,:,:] * mask_c[t_c,:,:].to(device) + image_fft_i[:,t_p,:,:] * mask[t_p,:,:]\n",
    "\n",
    "        ## Calculate the Loss term\n",
    "        Loss_mse = criterion(Predition,image_fft_out_unmasked)\n",
    "        Loss_real_even = criterion(Predition[:,0,:,:],torch.rot90(Predition[:,0,:,:],2,[1,2]))*10\n",
    "        Loss_imag_odd = criterion(Predition[:,1,:,:],-1*torch.rot90(Predition[:,1,:,:],2,[1,2]))*10\n",
    "        complexPrediction  = Predition[:,0,:,:][:,np.newaxis,:,:]+Predition[:,1,:,:][:,np.newaxis,:,:]*1j\n",
    "        complexPrediction = torch.fft.ifftshift(complexPrediction,dim=(2,3))\n",
    "        image_Predition = torch.fft.ifft2(complexPrediction)\n",
    "        image_Predition = torch.real(torch.tensor(image_Predition).float())\n",
    "        Loss_img = criterion(image_Predition,image[:,t_p,:,:][:,np.newaxis,:,:].to(device)) * 30000\n",
    "        Loss = Loss_mse + Loss_real_even + Loss_imag_odd + Loss_img\n",
    "        ## Calculate gradients for G\n",
    "        Loss.backward()\n",
    "\n",
    "        ## Optimization Step\n",
    "        optimizer.step()\n",
    "        \n",
    "        ## Print the Loss value after some iteration\n",
    "        with torch.no_grad():\n",
    "            if i % 10 == 0:\n",
    "                psnrValue = psnr(image_Predition.cpu(), image[:,t_p,:,:][:,np.newaxis,:,:].cpu())\n",
    "                ssimValue = ssim(image_Predition.cpu(), image[:,t_p,:,:][:,np.newaxis,:,:].cpu())\n",
    "                print(f'Epoch: {epoch}, Iteration: [{len(dataloader)}|{i}], Loss_mse: {Loss_mse:.4f}, Loss_real_even: {Loss_real_even:.4f}, Loss_imag_odd: {Loss_imag_odd:.4f}, Loss_img: {Loss_img:.4f}')\n",
    "                print(f'Epoch: {epoch}, Iteration: [{len(dataloader)}|{i}], psnr: {psnrValue:.4f}, ssim: {ssimValue:.4f}, max_GT: {torch.max(image[:,t_p,:,:][:,np.newaxis,:,:].cpu()):.4f}, max_Pred: {torch.max(image_Predition.cpu()):.4f},  min_Pred: {torch.min(image_Predition.cpu()):.4f}')\n",
    "                PSNR.append(psnrValue.item())\n",
    "                SSIM.append(ssimValue.item())\n",
    "                MSE_FFT.append(Loss_mse.item())\n",
    "                MSE_IMG.append(Loss_img.item())\n",
    "                MSE_RealEven.append(Loss_real_even.item())\n",
    "                MSE_ImagOdd.append(Loss_imag_odd.item())\n",
    "\n",
    "        \n",
    "        ## show the predited image\n",
    "        model.train(False)\n",
    "        with torch.no_grad():\n",
    "            if (i % 100 == 0) or (i == len(dataloader) - 1):\n",
    "                \n",
    "                imshow(vutils.make_grid(image_Predition.cpu(), padding=2, nrow=5, normalize=False), batch_size, '{}_{}_fix'.format(epoch, i))\n",
    "                imshow(vutils.make_grid(image[:,t_p,:,:][:,np.newaxis,:,:].cpu(), padding=2, nrow=5, normalize=False), batch_size, '{}_{}_GroundTruth'.format(epoch, i))\n",
    "            \n",
    "                PSNR_Vt = 0\n",
    "                SSIM_Vt = 0\n",
    "                MSE_FFT_Vt = 0\n",
    "                MSE_IMG_Vt = 0\n",
    "                MSE_RealEven_Vt = 0\n",
    "                MSE_ImagOdd_Vt  = 0\n",
    "            \n",
    "                print('Calculating loss and metrics for Validation dataset ... ')\n",
    "                for l, data_val in tqdm(enumerate(dataloader_val, 0)):\n",
    "                    # load one batch of data\n",
    "\n",
    "                    image_fft_r, image_fft_i, image, LayerNum, PosEncoding = data \n",
    "                    \n",
    "                    image_fft_out_unmasked = torch.concat((image_fft_r[:,t_p,:,:][:,np.newaxis,:,:],image_fft_i[:,t_p,:,:][:,np.newaxis,:,:]), dim = 1).to(device)\n",
    "\n",
    "#                     for i in range(6):\n",
    "#                             image_fft_r[:,i,:,:] = image_fft_r[:,i,:,:] * mask[i,:,:].cpu()\n",
    "#                             image_fft_i[:,i,:,:] = image_fft_i[:,i,:,:] * mask[i,:,:].cpu()\n",
    "\n",
    "                    # ListConChIdxs = [0,1,2,3,5] ## index 1 is the T1 middle layer that should be predicted\n",
    "                    ListConChIdxs = [0,2,3,t_c,5] ## index 1 is the T1 middle layer that should be predicted\n",
    "                    image_fft_in = torch.concat((image_fft_r[:,ListConChIdxs,:,:],image_fft_i[:,ListConChIdxs,:,:]), dim = 1).to(device)\n",
    "                    image_fft_out = torch.concat((image_fft_r[:,t_p,:,:][:,np.newaxis,:,:],image_fft_i[:,t_p,:,:][:,np.newaxis,:,:]), dim = 1).to(device)\n",
    "\n",
    "                    PosEncoding = PosEncoding.to(device)\n",
    "                    ## Inference\n",
    "                    Predition = model(image_fft_in.float(),PosEncoding.float(),LayerNum.to(device))\n",
    "                              \n",
    "                    # correct values of estimated image using true ones\n",
    "                    # Predition[:,0,:,:] = Predition[:,0,:,:] * mask_c[t_p,:,:] + image_fft_r[:,t_p,:,:].to(device) * mask[t_p,:,:]\n",
    "                    # Predition[:,1,:,:] = Predition[:,1,:,:] * mask_c[t_p,:,:] + image_fft_i[:,t_p,:,:].to(device) * mask[t_p,:,:]\n",
    "            \n",
    "                    ## Calculate the Loss term\n",
    "                    Loss_mse = criterion(Predition,image_fft_out_unmasked)\n",
    "                    Loss_real_even = criterion(Predition[:,0,:,:],torch.rot90(Predition[:,0,:,:],2,[1,2])) *10\n",
    "                    Loss_imag_odd = criterion(Predition[:,1,:,:],-1*torch.rot90(Predition[:,1,:,:],2,[1,2])) *10\n",
    "\n",
    "                    complexPrediction  = Predition[:,0,:,:][:,np.newaxis,:,:]+Predition[:,1,:,:][:,np.newaxis,:,:]*1j\n",
    "                    complexPrediction = torch.fft.ifftshift(complexPrediction,dim=(2,3))\n",
    "                    image_Predition = torch.fft.ifft2(complexPrediction)\n",
    "                    image_Predition = torch.real(torch.tensor(image_Predition).float())\n",
    "                    Loss_img = criterion(image_Predition,image[:,t_p,:,:][:,np.newaxis,:,:].to(device)) * 30000\n",
    "\n",
    "                    psnrValue = psnr(image_Predition.cpu(), image[:,t_p,:,:][:,np.newaxis,:,:].cpu())\n",
    "                    ssimValue = ssim(image_Predition.cpu(), image[:,t_p,:,:][:,np.newaxis,:,:].cpu())\n",
    "\n",
    "                    PSNR_Vt = PSNR_Vt + psnrValue.item()\n",
    "                    SSIM_Vt = SSIM_Vt + ssimValue.item()\n",
    "                    MSE_FFT_Vt = MSE_FFT_Vt + Loss_mse.item()\n",
    "                    MSE_IMG_Vt = MSE_IMG_Vt + Loss_img.item()\n",
    "                    MSE_RealEven_Vt = MSE_RealEven_Vt + Loss_real_even.item()\n",
    "                    MSE_ImagOdd_Vt  = MSE_ImagOdd_Vt + Loss_imag_odd.item()\n",
    "\n",
    "                normalizing_factor = len(dataloader_val)\n",
    "\n",
    "                PSNR_V.append(PSNR_Vt/normalizing_factor)\n",
    "                SSIM_V.append(SSIM_Vt/normalizing_factor)\n",
    "                MSE_FFT_V.append(MSE_FFT_Vt/normalizing_factor)\n",
    "                MSE_IMG_V.append(MSE_IMG_Vt/normalizing_factor)\n",
    "                MSE_RealEven_V.append(MSE_RealEven_Vt/normalizing_factor)\n",
    "                MSE_ImagOdd_V.append(MSE_ImagOdd_Vt/normalizing_factor)\n",
    "\n",
    "                mdic = {\"PSNR\": PSNR,\"SSIM\": SSIM,\"MSE_FFT\": MSE_FFT,\"MSE_IMG\": MSE_IMG,\"MSE_RealEven\": MSE_RealEven,\"MSE_ImagOdd\": MSE_ImagOdd\n",
    "                       ,\"PSNR_V\": PSNR_V,\"SSIM_V\": SSIM_V,\"MSE_FFT_V\": MSE_FFT_V,\"MSE_IMG_V\": MSE_IMG_V,\"MSE_RealEven_V\": MSE_RealEven_V,\"MSE_ImagOdd_V\": MSE_ImagOdd_V}\n",
    "                savemat(save_to_dir+\"LOG.mat\", mdic)\n",
    "                \n",
    "                ## to select best model of last epoch for next epoch training\n",
    "                if len(PSNR_V) > 1:\n",
    "                    print(f\"max:{np.max(PSNR_V[0:-1])},current:{PSNR_Vt/normalizing_factor}\")\n",
    "                    if PSNR_Vt/normalizing_factor > np.max(PSNR_V[0:-1]):\n",
    "                        print('new best model found!!saving...')\n",
    "                        torch.save(model, save_to_dir +'modelt1.pt')\n",
    "#                         torch.save({\n",
    "#                         'epoch': epoch,\n",
    "#                         'model_state_dict': model.state_dict(),\n",
    "#                         'optimizer_state_dict': optimizer.state_dict(),\n",
    "#                         'loss': Loss\n",
    "#                         }, save_to_dir +'modelt2.pt')\n",
    "\n",
    "                \n",
    "        ## Get the Layer Encodeing base on the layer number\n",
    "    scheduler.step()\n",
    "    print(scheduler.get_last_lr())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e148d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = np.random.rand(4,4)\n",
    "print(mat)\n",
    "fft = np.fft.fft2(mat)\n",
    "# fft = np.fft.fftshift(fft)\n",
    "print(fft.real)\n",
    "print(fft.imag)\n",
    "print(fft[1:,1:] - np.conj(np.rot90(fft[1:,1:],2)))\n",
    "\n",
    "# fft = np.fft.fftshift(fft)\n",
    "\n",
    "# print(fft.real)\n",
    "# print(fft.imag)\n",
    "\n",
    "# fft = np.fft.fftshift(fft)\n",
    "# pred = np.fft.ifft(fft,n=6)\n",
    "# print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c20fb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "v = np.arange(1)\n",
    "v\n",
    "np.max(v[0:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40b560",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda11_7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "dd42dc5079a55ff97ff57ff9b01ace7b918ee8431bba5bd199ac8f7faef5fc7a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
